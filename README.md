ğŸ“˜ Optimal Execution Engine
Almgrenâ€“Chriss â€¢ Market Impact Modeling â€¢ Reinforcement Learning

A full-stack quantitative trading research project implementing:

Almgrenâ€“Chriss optimal execution (closed-form solution)

Stochastic price simulation (GBM + OU processes)

Permanent & temporary market impact models

Reinforcement Learning execution agent (PPO / DQN)

Backtesting & evaluation suite

This project demonstrates stochastic calculus, microstructure modeling, optimal control, and machine learning for trading â€” ideal for quant interviews and research roles.

ğŸ“Œ Table of Contents

Motivation

Key Concepts

System Architecture

Project Structure

Installation & Usage

Detailed Module Explanation

Evaluation & Metrics

Extensions & Future Work

References

ğŸ’¡ Motivation

Executing a large order without moving the market is one of the most important problems in quant trading.
A naive execution can lead to:

high implementation shortfall

excessive market impact

liquidity-driven slippage

risk from price volatility

This project builds an Optimal Execution Engine that:

models market dynamics using SDEs,

analytically computes the optimal execution trajectory using Almgrenâ€“Chriss,

trains an RL agent to beat the analytical strategy in richer market environments.

ğŸ§  Key Concepts

This section covers the math and intuition behind the project.

1ï¸âƒ£ Price Dynamics â€” Stochastic Differential Equations (SDEs)

The mid-price process 
ğ‘†
ğ‘¡
S
t
	â€‹

 can follow:

Geometric Brownian Motion (GBM)
ğ‘‘
ğ‘†
ğ‘¡
=
ğœ‡
ğ‘†
ğ‘¡
ğ‘‘
ğ‘¡
+
ğœ
ğ‘†
ğ‘¡
ğ‘‘
ğ‘Š
ğ‘¡
dS
t
	â€‹

=Î¼S
t
	â€‹

dt+ÏƒS
t
	â€‹

dW
t
	â€‹

Arithmetic Brownian Motion (ABM)
ğ‘‘
ğ‘†
ğ‘¡
=
ğœ‡
ğ‘‘
ğ‘¡
+
ğœ
ğ‘‘
ğ‘Š
ğ‘¡
dS
t
	â€‹

=Î¼dt+ÏƒdW
t
	â€‹


Why?
Short-horizon intraday prices behave nearly linearly (ABM) but longer intraday periods sometimes fit GBM.

2ï¸âƒ£ Microstructure Alpha â€” Ornsteinâ€“Uhlenbeck (OU) Process

Models short-term mean-reversion in order flow:

ğ‘‘
ğ‘‹
ğ‘¡
=
âˆ’
ğœƒ
ğ‘‹
ğ‘¡
ğ‘‘
ğ‘¡
+
ğœ‚
ğ‘‘
ğ‘Š
ğ‘¡
dX
t
	â€‹

=âˆ’Î¸X
t
	â€‹

dt+Î·dW
t
	â€‹


This provides RL with an exploitable alpha signal.

3ï¸âƒ£ Market Impact Modeling

Trading affects the market in two ways:

Permanent Impact
ğ‘†
ğ‘¡
perm
=
ğ‘†
ğ‘¡
âˆ’
1
+
ğ›¾
ğ‘£
ğ‘¡
S
t
perm
	â€‹

=S
tâˆ’1
	â€‹

+Î³v
t
	â€‹

Temporary Impact
ğ¶
ğ‘¡
=
ğ‘†
ğ‘¡
+
ğœ–
ğ‘£
ğ‘¡
C
t
	â€‹

=S
t
	â€‹

+Ïµv
t
	â€‹


Where:

ğ‘£
ğ‘¡
v
t
	â€‹

 = shares traded at time t

ğ›¾
Î³ = permanent impact coefficient

ğœ–
Ïµ = temporary impact coefficient

Modeling impact is essential for realistic execution.

4ï¸âƒ£ Almgrenâ€“Chriss Optimal Execution

The classical solution solves:

min
â¡
ğ‘£
ğ‘¡
ğ¸
[
Cost
]
+
ğœ†
â‹…
Risk
v
t
	â€‹

min
	â€‹

E[Cost]+Î»â‹…Risk

Closed-form optimal trading trajectory:

ğ‘¥
ğ‘¡
=
ğ‘‹
0
â‹…
sinh
â¡
(
ğ‘˜
(
ğ‘‡
âˆ’
ğ‘¡
)
)
sinh
â¡
(
ğ‘˜
ğ‘‡
)
x
t
	â€‹

=X
0
	â€‹

â‹…
sinh(kT)
sinh(k(Tâˆ’t))
	â€‹


Where:

ğ‘‹
0
X
0
	â€‹

: total shares

ğ‘˜
k: derived from volatility & impact

ğ‘‡
T: trading horizon

This provides a baseline to compare RL vs optimal control.

5ï¸âƒ£ Reinforcement Learning Execution Agent

The agent observes:

mid-price

OU signal

volatility

liquidity

remaining inventory

remaining time

Objective: minimize implementation shortfall.

RL Algorithms Supported:

PPO (default)

DQN

A2C

The agent often outperforms Almgrenâ€“Chriss in markets with stochastic alpha or liquidity shocks.

ğŸ—ï¸ System Architecture
                â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                â”‚       Market Simulator        â”‚
                â”‚  (GBM + OU + Impact + Liquidity)
                â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                               â”‚
                â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                â”‚   Execution Environment       â”‚
                â”‚   (Gym-style RL environment)  â”‚
                â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                               â”‚
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚                          â”‚                          â”‚
â”Œâ”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  TWAP/VWAP   â”‚      â”‚ Almgrenâ€“Chriss    â”‚      â”‚ RL Agent (PPO)   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                               â”‚
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚ Backtesting Engine â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                               â”‚
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚   Metrics & Plots   â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

ğŸ“ Project Structure
Optimal-Execution-Engine/
â”‚
â”œâ”€â”€ env/                # Market simulator + Gym environment
â”œâ”€â”€ market_simulator/   # SDEs, impact models, liquidity models
â”œâ”€â”€ rl_agent/           # PPO / DQN RL agents
â”œâ”€â”€ evaluation/         # Backtesting, metrics, plotting
â”œâ”€â”€ utils/              # Helper functions
â”œâ”€â”€ main.py             # Train + evaluate pipeline
â””â”€â”€ README.md

âš™ï¸ Installation & Usage
1. Clone the repo
git clone https://github.com/sohanrajr-sys/Optimal-Execution-Engine-Using-Almgren-Chriss-Market-Impact-Modeling-and-Reinforcement-Learning
cd Optimal-Execution-Engine-Using-Almgren-Chriss-Market-Impact-Modeling-and-Reinforcement-Learning

2. Install requirements
pip install -r requirements.txt

3. Run baseline + RL training
python main.py

4. View results

Plots and logs will appear in:

/results

ğŸ“˜ Detailed Module Explanation
ğŸ“‚ market_simulator/

Implements:

GBM / ABM price SDE

OU alpha signal

temporary & permanent impact

stochastic liquidity

Monte Carlo simulation

Generates realistic trajectories for execution.

ğŸ“‚ env/

A Gym-like environment where the RL agent interacts with the market.

State includes:

price

alpha

remaining shares

remaining time

market depth

Actions: number of shares to execute.

Reward = negative execution cost.

ğŸ“‚ rl_agent/

Implementation of:

PPO

DQN

A2C

with:

policy networks

replay buffers

training loops

exploration strategies

ğŸ“‚ evaluation/

Measures:

implementation shortfall

realized cost

variance of cost

PnL distribution

Sharpe-style risk-adjusted measures

Also provides plotting utilities for:

execution paths

price trajectories

policy comparison

ğŸ“Š Evaluation & Metrics

Metrics include:

1. Implementation Shortfall (IS)
IS
=
âˆ‘
(
ğ‘
ğ‘¡
âˆ’
ğ‘
0
)
ğ‘£
ğ‘¡
IS=âˆ‘(p
t
	â€‹

âˆ’p
0
	â€‹

)v
t
	â€‹

2. Trading Cost Decomposition

Temporary impact cost

Permanent impact cost

Drift cost

Volatility risk

3. Strategy Comparison

TWAP

VWAP

Almgrenâ€“Chriss

RL agent

4. Monte Carlo Backtesting

Thousands of simulated paths for robust statistics.

ğŸš€ Extensions & Future Work

You can extend this project into deeper quant research by adding:

Heston volatility model

queue-reactive limit order book simulator

Jump-diffusion price process

Deep RL with attention networks

Multi-asset execution

Adversarial market maker simulation

ğŸ“š References

Almgren, R., & Chriss, N. (2000). Optimal execution of portfolio transactions.

Gatheral, J. The Volatility Surface.

Cartea, Ã., Jaimungal, S., & Penalva, J. Algorithmic and High-Frequency Trading.

Bertsimas & Lo. Optimal control of execution costs.

Sutton & Barto. Reinforcement Learning: An Introduction.
